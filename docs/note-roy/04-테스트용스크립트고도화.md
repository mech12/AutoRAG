# 테스트용 스크립트 고도화 계획

## 현재 문제점

현재 `make prepare-data`와 `make evaluate-custom` 명령어는 폴더가 고정되어 있어서:

- 테스트 데이터나 설정이 바뀌면 변경이 어려움
- 여러 테스트 케이스를 관리하기 불편
- 설정 재현성이 떨어짐

```bash
# 현재 방식 - 폴더가 고정됨
make prepare-data INPUT_DIR=docs/sample-data  # -> data/custom/
make evaluate-custom                           # -> logs/
```

## 제안: 테스트 케이스 기반 관리

### 설정 파일 구조

**`scripts/test-config.yaml`** (JSON보다 YAML이 주석 지원으로 더 적합)

```yaml
# 테스트 케이스 설정
test_cases:
  인사규정:
    # 입력 설정
    input_dir: "docs/sample-data"

    # 데이터 준비 설정
    num_qa: 20
    use_llm: false
    chunk_size: 512
    chunk_overlap: 50

    # RAG 평가 설정
    rag_config: "sample_config/rag/korean/non_gpu/simple_korean_custom.yaml"

    # 출력 설정 (모든 결과가 이 폴더 하위에 생성)
    output_dir: "logs/인사규정"

  회계규정:
    input_dir: "docs/sample-data/회계"
    num_qa: 50
    use_llm: true
    chunk_size: 1024
    chunk_overlap: 100
    rag_config: "sample_config/rag/korean/non_gpu/full_korean_custom.yaml"
    output_dir: "logs/회계규정"

# 기본값 (테스트 케이스에서 생략 시 적용)
defaults:
  num_qa: 20
  use_llm: false
  chunk_size: 512
  chunk_overlap: 50
  rag_config: "sample_config/rag/korean/non_gpu/simple_korean_custom.yaml"
```

### 출력 폴더 구조

```text
logs/인사규정/
├── data/                    # prepare-data 결과
│   ├── corpus.parquet
│   ├── qa.parquet
│   ├── parse_config.yaml
│   ├── chunk_config.yaml
│   ├── parse_project/
│   └── chunk_project/
├── trial/                   # evaluate-custom 결과
│   ├── 0/
│   │   ├── summary.csv
│   │   ├── config.yaml
│   │   ├── retrieve_node_line/
│   │   └── post_retrieve_node_line/
│   └── trial.json
└── test-config.yaml         # 사용된 설정 복사본 (재현성)
```

### 사용법

```bash
# 테스트 케이스 목록 확인
make list-testcases

# 특정 테스트 케이스로 데이터 준비
make prepare-data TESTCASE=인사규정

# 특정 테스트 케이스로 평가 실행
make evaluate-custom TESTCASE=인사규정

# 전체 워크플로우 (prepare + evaluate)
make run-testcase TESTCASE=인사규정

# 모든 테스트 케이스 실행
make run-all-testcases

# 테스트 케이스 결과 비교
make compare-results
```

## 구현 계획

### 1단계: 설정 파일 파서 구현

**`scripts/testcase_config.py`**

```python
import yaml
from dataclasses import dataclass
from pathlib import Path

@dataclass
class TestCase:
    name: str
    input_dir: str
    output_dir: str
    num_qa: int = 20
    use_llm: bool = False
    chunk_size: int = 512
    chunk_overlap: int = 50
    rag_config: str = "sample_config/rag/korean/non_gpu/simple_korean_custom.yaml"

def load_testcase(name: str, config_path: str = "scripts/test-config.yaml") -> TestCase:
    """테스트 케이스 로드"""
    with open(config_path) as f:
        config = yaml.safe_load(f)

    defaults = config.get("defaults", {})
    case = config["test_cases"].get(name)

    if not case:
        raise ValueError(f"테스트 케이스 '{name}'을 찾을 수 없습니다.")

    # defaults와 병합
    merged = {**defaults, **case, "name": name}
    return TestCase(**merged)

def list_testcases(config_path: str = "scripts/test-config.yaml") -> list:
    """모든 테스트 케이스 목록"""
    with open(config_path) as f:
        config = yaml.safe_load(f)
    return list(config["test_cases"].keys())
```

### 2단계: prepare_custom_data.py 수정

```python
# 기존 argparse에 --testcase 옵션 추가
parser.add_argument(
    "--testcase",
    type=str,
    help="테스트 케이스 이름 (scripts/test-config.yaml에서 로드)"
)

# testcase가 지정되면 설정 파일에서 로드
if args.testcase:
    tc = load_testcase(args.testcase)
    args.input_dir = tc.input_dir
    args.output_dir = f"{tc.output_dir}/data"
    args.num_qa = tc.num_qa
    args.use_llm = tc.use_llm
```

### 3단계: Makefile 수정

```makefile
# 테스트 케이스 설정 파일
TEST_CONFIG := scripts/test-config.yaml

# 테스트 케이스 목록
list-testcases: ## List all available test cases
    @$(PYTHON) -c "from scripts.testcase_config import list_testcases; \
        cases = list_testcases(); \
        print('Available test cases:'); \
        [print(f'  - {c}') for c in cases]"

# 테스트 케이스 기반 데이터 준비
prepare-data-tc: ## Prepare data for a test case (TESTCASE required)
    @if [ -z "$(TESTCASE)" ]; then echo "Usage: make prepare-data-tc TESTCASE=<name>"; exit 1; fi
    $(PYTHON) scripts/prepare_custom_data.py --testcase $(TESTCASE)

# 테스트 케이스 기반 평가
evaluate-tc: ## Evaluate a test case (TESTCASE required)
    @if [ -z "$(TESTCASE)" ]; then echo "Usage: make evaluate-tc TESTCASE=<name>"; exit 1; fi
    $(PYTHON) scripts/run_evaluation.py --testcase $(TESTCASE)

# 전체 워크플로우
run-testcase: prepare-data-tc evaluate-tc ## Run full workflow for a test case
```

### 4단계: 결과 비교 도구

**`scripts/compare_results.py`**

```python
def compare_testcases(testcases: list[str]):
    """여러 테스트 케이스 결과 비교"""
    results = []
    for tc_name in testcases:
        tc = load_testcase(tc_name)
        summary_path = f"{tc.output_dir}/trial/0/summary.csv"
        if os.path.exists(summary_path):
            df = pd.read_csv(summary_path)
            results.append({"testcase": tc_name, **extract_metrics(df)})

    # 비교 테이블 출력
    comparison_df = pd.DataFrame(results)
    print(comparison_df.to_markdown())
```

## 작업 항목

| 순서 | 작업 | 설명 | 예상 시간 |
| ---- | ---- | ---- | --------- |
| 1 | `scripts/test-config.yaml` 생성 | 기본 테스트 케이스 설정 파일 | 10분 |
| 2 | `scripts/testcase_config.py` 구현 | 설정 파일 파서 | 30분 |
| 3 | `prepare_custom_data.py` 수정 | --testcase 옵션 추가 | 20분 |
| 4 | `scripts/run_evaluation.py` 생성 | 평가 실행 래퍼 스크립트 | 30분 |
| 5 | Makefile 수정 | 새 타겟 추가 | 20분 |
| 6 | `scripts/compare_results.py` 구현 | 결과 비교 도구 | 30분 |
| 7 | 문서 및 도움말 업데이트 | help-testcase 타겟 등 | 20분 |

## 추가 고려사항

### 환경변수 오버라이드

테스트 케이스별로 다른 LLM/임베딩 서버를 사용할 수 있도록:

```yaml
test_cases:
  인사규정_gpu:
    input_dir: "docs/sample-data"
    output_dir: "logs/인사규정_gpu"
    # 환경변수 오버라이드
    env:
      CUSTOM_LLM_API_BASE: "http://gpu-server:8000/v1"
      CUSTOM_LLM_MODEL: "llama-70b"
```

### 재현성 보장

- 테스트 실행 시 사용된 설정을 출력 폴더에 복사
- Git commit hash 기록
- 실행 시간 및 환경 정보 기록

### CI/CD 통합

```yaml
# .github/workflows/test.yaml
- name: Run test cases
  run: |
    make prepare-data-tc TESTCASE=인사규정
    make evaluate-tc TESTCASE=인사규정
```

## 결론

이 구조를 통해:

1. **재현성**: 설정 파일로 테스트 재현 가능
2. **관리 용이성**: 여러 테스트 케이스를 한 곳에서 관리
3. **비교 분석**: 서로 다른 설정의 결과를 쉽게 비교
4. **확장성**: 새 테스트 케이스 추가가 간단

## 다음 단계

구현을 진행하려면:

```bash
# 이 계획을 승인하고 구현 시작
# 1단계부터 순차적으로 진행
```
