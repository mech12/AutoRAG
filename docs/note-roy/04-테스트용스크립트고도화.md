# 테스트용 스크립트 고도화 계획

## 현재 샘플 데이터 구조

```text
docs/sample-data/
├── 인사규정/
│   └── 인사규정-07-취업규칙.pdf      (507KB)
└── 고압가스/
    └── 기술기준-07-FU212_특수고압가스사용.pdf  (1.4MB)
```

## 현재 문제점

현재 `make prepare-data`와 `make evaluate-custom` 명령어는 폴더가 고정되어 있어서:

- 테스트 데이터나 설정이 바뀌면 변경이 어려움
- 여러 테스트 케이스를 관리하기 불편
- 설정 재현성이 떨어짐

```bash
# 현재 방식 - 폴더가 고정됨
make prepare-data INPUT_DIR=docs/sample-data/인사규정  # -> data/custom/
make evaluate-custom                                    # -> logs/
```

## 제안: 테스트 케이스 기반 관리

### 설정 파일 구조

**`scripts/test-config.yaml`**

```yaml
# =============================================================================
# AutoRAG 테스트 케이스 설정
# =============================================================================
# 사용법:
#   make list-testcases                    # 테스트 케이스 목록
#   make run-testcase TESTCASE=인사규정    # 전체 워크플로우 실행
# =============================================================================

# 기본값 (테스트 케이스에서 생략 시 적용)
defaults:
  num_qa: 20
  use_llm: false
  chunk_size: 512
  chunk_overlap: 50
  rag_config: "sample_config/rag/korean/non_gpu/simple_korean_custom.yaml"

# 테스트 케이스 정의
test_cases:
  # -------------------------------------------------------------------------
  # 인사규정 테스트 케이스
  # -------------------------------------------------------------------------
  인사규정:
    description: "취업규칙 PDF 기반 RAG 테스트"
    input_dir: "docs/sample-data/인사규정"
    output_dir: "logs/인사규정"
    # 기본값 사용: num_qa=20, use_llm=false, chunk_size=512

  인사규정_llm:
    description: "취업규칙 - LLM 사용 고품질 QA 생성"
    input_dir: "docs/sample-data/인사규정"
    output_dir: "logs/인사규정_llm"
    num_qa: 50
    use_llm: true

  # -------------------------------------------------------------------------
  # 고압가스 테스트 케이스
  # -------------------------------------------------------------------------
  고압가스:
    description: "특수고압가스 기술기준 PDF 기반 RAG 테스트"
    input_dir: "docs/sample-data/고압가스"
    output_dir: "logs/고압가스"
    chunk_size: 1024      # 기술 문서는 더 큰 청크 사용
    chunk_overlap: 100

  고압가스_상세:
    description: "고압가스 - 더 많은 QA 생성"
    input_dir: "docs/sample-data/고압가스"
    output_dir: "logs/고압가스_상세"
    num_qa: 100
    use_llm: true
    chunk_size: 1024
    chunk_overlap: 100

  # -------------------------------------------------------------------------
  # 전체 테스트 케이스 (모든 샘플 데이터)
  # -------------------------------------------------------------------------
  전체:
    description: "모든 샘플 PDF 통합 테스트"
    input_dir: "docs/sample-data"
    output_dir: "logs/전체"
    num_qa: 100
    recursive: true  # 하위 폴더 포함
```

### 출력 폴더 구조

```text
logs/
├── 인사규정/                      # TESTCASE=인사규정
│   ├── data/                      # prepare-data 결과
│   │   ├── corpus.parquet         # 48 chunks (예시)
│   │   ├── qa.parquet             # 20 QA pairs
│   │   ├── parse_config.yaml
│   │   ├── chunk_config.yaml
│   │   ├── parse_project/
│   │   └── chunk_project/
│   ├── trial/                     # evaluate-custom 결과
│   │   ├── 0/
│   │   │   ├── summary.csv
│   │   │   ├── config.yaml
│   │   │   ├── retrieve_node_line/
│   │   │   └── post_retrieve_node_line/
│   │   └── trial.json
│   └── testcase.yaml              # 사용된 설정 복사본
│
├── 고압가스/                      # TESTCASE=고압가스
│   └── ...
│
└── 전체/                          # TESTCASE=전체
    └── ...
```

### 사용법

```bash
# 테스트 케이스 목록 확인
make list-testcases

# 출력 예시:
# Available test cases:
#   - 인사규정      : 취업규칙 PDF 기반 RAG 테스트
#   - 인사규정_llm  : 취업규칙 - LLM 사용 고품질 QA 생성
#   - 고압가스      : 특수고압가스 기술기준 PDF 기반 RAG 테스트
#   - 고압가스_상세 : 고압가스 - 더 많은 QA 생성
#   - 전체         : 모든 샘플 PDF 통합 테스트

# 특정 테스트 케이스 실행
make run-testcase TESTCASE=인사규정

# 개별 단계 실행
make prepare-data TESTCASE=인사규정
make evaluate-custom TESTCASE=인사규정

# 결과 비교
make compare-results

# 출력 예시:
# | testcase    | retrieval_f1 | retrieval_recall | rouge  |
# |-------------|--------------|------------------|--------|
# | 인사규정    | 0.50         | 1.00             | 0.41   |
# | 고압가스    | 0.45         | 0.95             | 0.38   |
```

## 구현 계획

### 1단계: 설정 파일 파서 구현

**`scripts/testcase_config.py`**

```python
#!/usr/bin/env python3
"""테스트 케이스 설정 파일 파서"""

import yaml
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

@dataclass
class TestCase:
    """테스트 케이스 설정"""
    name: str
    input_dir: str
    output_dir: str
    description: str = ""
    num_qa: int = 20
    use_llm: bool = False
    chunk_size: int = 512
    chunk_overlap: int = 50
    recursive: bool = False
    rag_config: str = "sample_config/rag/korean/non_gpu/simple_korean_custom.yaml"
    env: dict = field(default_factory=dict)

    @property
    def data_dir(self) -> str:
        """데이터 출력 디렉토리"""
        return f"{self.output_dir}/data"

    @property
    def trial_dir(self) -> str:
        """평가 결과 디렉토리"""
        return f"{self.output_dir}/trial"

def load_config(config_path: str = "scripts/test-config.yaml") -> dict:
    """설정 파일 로드"""
    with open(config_path) as f:
        return yaml.safe_load(f)

def load_testcase(name: str, config_path: str = "scripts/test-config.yaml") -> TestCase:
    """테스트 케이스 로드"""
    config = load_config(config_path)
    defaults = config.get("defaults", {})
    case = config["test_cases"].get(name)

    if not case:
        available = list(config["test_cases"].keys())
        raise ValueError(
            f"테스트 케이스 '{name}'을 찾을 수 없습니다.\n"
            f"사용 가능한 케이스: {', '.join(available)}"
        )

    # defaults와 병합
    merged = {**defaults, **case, "name": name}
    return TestCase(**{k: v for k, v in merged.items() if k in TestCase.__dataclass_fields__})

def list_testcases(config_path: str = "scripts/test-config.yaml") -> list[tuple[str, str]]:
    """모든 테스트 케이스 목록 (이름, 설명)"""
    config = load_config(config_path)
    return [
        (name, case.get("description", ""))
        for name, case in config["test_cases"].items()
    ]

if __name__ == "__main__":
    # CLI로 실행 시 목록 출력
    import sys
    if len(sys.argv) > 1:
        tc = load_testcase(sys.argv[1])
        print(f"Name: {tc.name}")
        print(f"Input: {tc.input_dir}")
        print(f"Output: {tc.output_dir}")
    else:
        print("Available test cases:")
        for name, desc in list_testcases():
            print(f"  - {name:15} : {desc}")
```

### 2단계: prepare_custom_data.py 수정

```python
# 기존 argparse에 --testcase 옵션 추가
parser.add_argument(
    "--testcase",
    type=str,
    help="테스트 케이스 이름 (scripts/test-config.yaml에서 로드)"
)

def main():
    args = parser.parse_args()

    # testcase가 지정되면 설정 파일에서 로드
    if args.testcase:
        from testcase_config import load_testcase
        tc = load_testcase(args.testcase)
        args.input_dir = tc.input_dir
        args.output_dir = tc.data_dir
        args.num_qa = tc.num_qa
        args.use_llm = tc.use_llm
        # chunk_size, chunk_overlap도 적용
```

### 3단계: run_evaluation.py 생성

**`scripts/run_evaluation.py`**

```python
#!/usr/bin/env python3
"""테스트 케이스 기반 RAG 평가 실행"""

import os
import sys
import argparse
import subprocess
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))
from testcase_config import load_testcase

def run_evaluation(testcase: str):
    """테스트 케이스로 평가 실행"""
    tc = load_testcase(testcase)

    # 환경변수 오버라이드
    env = os.environ.copy()
    env.update(tc.env)

    cmd = [
        "autorag", "evaluate",
        "--config", tc.rag_config,
        "--qa_data_path", f"{tc.data_dir}/qa.parquet",
        "--corpus_data_path", f"{tc.data_dir}/corpus.parquet",
        "--project_dir", tc.trial_dir,
    ]

    print(f"Running: {' '.join(cmd)}")
    subprocess.run(cmd, env=env, check=True)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--testcase", required=True)
    args = parser.parse_args()
    run_evaluation(args.testcase)
```

### 4단계: Makefile 수정

```makefile
##@ Test Cases
list-testcases: ## List all available test cases
	@$(PYTHON) scripts/testcase_config.py

# TESTCASE 파라미터가 있으면 testcase 모드로 실행
prepare-data: ## Prepare data (INPUT_DIR or TESTCASE)
ifdef TESTCASE
	$(PYTHON) scripts/prepare_custom_data.py --testcase $(TESTCASE)
else ifdef INPUT_DIR
	$(PYTHON) scripts/prepare_custom_data.py \
		--input_dir $(INPUT_DIR) \
		--output_dir $(or $(OUTPUT_DIR),data/custom) \
		--num_qa $(or $(NUM_QA),20) \
		$(if $(filter true,$(USE_LLM)),--use_llm,)
else
	@echo "Usage:"
	@echo "  make prepare-data TESTCASE=<name>"
	@echo "  make prepare-data INPUT_DIR=<path>"
	@exit 1
endif

evaluate-custom: ## Evaluate (TESTCASE or default)
ifdef TESTCASE
	$(PYTHON) scripts/run_evaluation.py --testcase $(TESTCASE)
else
	@if [ ! -f .env ]; then echo "Error: .env not found"; exit 1; fi
	autorag evaluate \
		--config $(CONFIG_CUSTOM) \
		--qa_data_path data/custom/qa.parquet \
		--corpus_data_path data/custom/corpus.parquet \
		--project_dir $(PROJECT_DIR)
endif

run-testcase: ## Run full workflow (TESTCASE required)
	@if [ -z "$(TESTCASE)" ]; then echo "Usage: make run-testcase TESTCASE=<name>"; exit 1; fi
	$(MAKE) prepare-data TESTCASE=$(TESTCASE)
	$(MAKE) evaluate-custom TESTCASE=$(TESTCASE)

compare-results: ## Compare all test case results
	@$(PYTHON) scripts/compare_results.py
```

### 5단계: 결과 비교 도구

**`scripts/compare_results.py`**

```python
#!/usr/bin/env python3
"""테스트 케이스 결과 비교"""

import os
import pandas as pd
from pathlib import Path
from testcase_config import list_testcases, load_testcase

def extract_metrics(summary_path: str) -> dict:
    """summary.csv에서 주요 메트릭 추출"""
    df = pd.read_csv(summary_path)

    metrics = {}
    for _, row in df.iterrows():
        node = row["node_type"]
        if node == "semantic_retrieval":
            # retrieval metrics는 별도 파일에서
            pass
        elif node == "generator":
            metrics["execution_time"] = row["best_execution_time"]

    return metrics

def compare_all():
    """모든 테스트 케이스 결과 비교"""
    results = []

    for name, desc in list_testcases():
        tc = load_testcase(name)
        summary_path = f"{tc.trial_dir}/0/summary.csv"

        if os.path.exists(summary_path):
            metrics = {"testcase": name, "description": desc}

            # Retrieval metrics
            retrieval_path = f"{tc.trial_dir}/0/retrieve_node_line/semantic_retrieval/summary.csv"
            if os.path.exists(retrieval_path):
                ret_df = pd.read_csv(retrieval_path)
                metrics["retrieval_f1"] = ret_df["retrieval_f1"].iloc[0]
                metrics["retrieval_recall"] = ret_df["retrieval_recall"].iloc[0]

            # Generator metrics
            gen_path = f"{tc.trial_dir}/0/post_retrieve_node_line/generator/summary.csv"
            if os.path.exists(gen_path):
                gen_df = pd.read_csv(gen_path)
                metrics["rouge"] = gen_df["rouge"].iloc[0]
                metrics["exec_time"] = gen_df["execution_time"].iloc[0]

            results.append(metrics)

    if results:
        df = pd.DataFrame(results)
        print("\n" + "=" * 70)
        print("테스트 케이스 결과 비교")
        print("=" * 70)
        print(df.to_markdown(index=False, floatfmt=".3f"))
    else:
        print("실행된 테스트 케이스가 없습니다.")

if __name__ == "__main__":
    compare_all()
```

## 작업 항목

| 순서 | 작업 | 파일 | 예상 시간 |
| ---- | ---- | ---- | --------- |
| 1 | 설정 파일 생성 | `scripts/test-config.yaml` | 10분 |
| 2 | 설정 파서 구현 | `scripts/testcase_config.py` | 20분 |
| 3 | prepare 스크립트 수정 | `scripts/prepare_custom_data.py` | 20분 |
| 4 | 평가 래퍼 생성 | `scripts/run_evaluation.py` | 20분 |
| 5 | 결과 비교 도구 | `scripts/compare_results.py` | 20분 |
| 6 | Makefile 수정 | `Makefile` | 20분 |
| 7 | 도움말 추가 | `make help-testcase` | 10분 |

**총 예상 시간: 약 2시간**

## 예상 사용 시나리오

### 시나리오 1: 새 테스트 케이스 추가

```yaml
# scripts/test-config.yaml에 추가
test_cases:
  새규정:
    description: "새로운 규정 문서 테스트"
    input_dir: "docs/sample-data/새규정"
    output_dir: "logs/새규정"
```

```bash
make run-testcase TESTCASE=새규정
```

### 시나리오 2: 청크 크기 비교 테스트

```yaml
test_cases:
  인사규정_chunk256:
    input_dir: "docs/sample-data/인사규정"
    output_dir: "logs/인사규정_chunk256"
    chunk_size: 256

  인사규정_chunk512:
    input_dir: "docs/sample-data/인사규정"
    output_dir: "logs/인사규정_chunk512"
    chunk_size: 512

  인사규정_chunk1024:
    input_dir: "docs/sample-data/인사규정"
    output_dir: "logs/인사규정_chunk1024"
    chunk_size: 1024
```

```bash
make run-testcase TESTCASE=인사규정_chunk256
make run-testcase TESTCASE=인사규정_chunk512
make run-testcase TESTCASE=인사규정_chunk1024
make compare-results
```

### 시나리오 3: 다른 LLM 서버로 테스트

```yaml
test_cases:
  인사규정_gpt4:
    input_dir: "docs/sample-data/인사규정"
    output_dir: "logs/인사규정_gpt4"
    env:
      CUSTOM_LLM_API_BASE: "https://api.openai.com/v1"
      CUSTOM_LLM_MODEL: "gpt-4"
```

## 결론

이 구조를 통해:

1. **재현성**: 설정 파일로 테스트 재현 가능
2. **관리 용이성**: 여러 테스트 케이스를 한 곳에서 관리
3. **비교 분석**: 서로 다른 설정의 결과를 쉽게 비교
4. **확장성**: 새 테스트 케이스 추가가 간단

## 다음 단계

구현을 진행하려면 다음 명령어 실행:

```bash
# 구현 시작
# (Claude에게 요청)
```
