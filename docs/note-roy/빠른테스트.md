# AutoRAG 빠른 테스트 가이드

## 실행 명령어

```bash
autorag evaluate \
  --config sample_config/rag/korean/non_gpu/simple_korean.yaml \
  --qa_data_path tests/resources/dataset_sample_gen_by_autorag/qa.parquet \
  --corpus_data_path tests/resources/dataset_sample_gen_by_autorag/corpus.parquet \
  --project_dir logs
```

---

## 명령어 구성 요소

| 옵션 | 설명 |
|------|------|
| `--config` | RAG 파이프라인 설정 YAML 파일 경로 |
| `--qa_data_path` | 평가용 QA 데이터셋 (질문-답변 쌍) |
| `--corpus_data_path` | 검색 대상 문서 코퍼스 |
| `--project_dir` | 결과 출력 디렉토리 (기본값: 현재 폴더) |

---

## 설정 파일 분석 (simple_korean.yaml)

```yaml
node_lines:
  - node_line_name: retrieve_node_line
    nodes:
      - node_type: semantic_retrieval      # 의미 기반 검색
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        top_k: 3                           # 상위 3개 문서 검색
        modules:
          - module_type: vectordb
            vectordb: default              # 기본 벡터DB 사용

  - node_line_name: post_retrieve_node_line
    nodes:
      - node_type: prompt_maker            # 프롬프트 생성
        strategy:
          metrics: [bleu, meteor, rouge]
        modules:
          - module_type: fstring
            prompt: "주어진 passage만을 이용하여 question에 따라 답하시오..."

      - node_type: generator               # 답변 생성
        strategy:
          metrics:
            - metric_name: rouge
            - metric_name: sem_score
              embedding_model: openai
            - metric_name: bert_score
              lang: ko
        modules:
          - module_type: llama_index_llm
            llm: openai
            model: [gpt-4o-mini]
```

### 파이프라인 흐름

```text
Query → Semantic Retrieval (top_k=3) → Prompt Maker → Generator (gpt-4o-mini) → Answer
```

### 사용되는 평가 메트릭

| 단계 | 메트릭 | 설명 |
|------|--------|------|
| **검색** | retrieval_f1 | 검색 정확도 F1 점수 |
| | retrieval_recall | 검색 재현율 |
| | retrieval_precision | 검색 정밀도 |
| **프롬프트** | bleu | 번역 품질 평가 |
| | meteor | 의미적 유사도 |
| | rouge | 요약 품질 평가 |
| **생성** | rouge | 텍스트 유사도 |
| | sem_score | 의미적 유사도 (OpenAI 임베딩) |
| | bert_score | BERT 기반 유사도 (한국어) |

---

## 샘플 데이터 구조

### QA 데이터 (20개 샘플)

| 컬럼 | 설명 | 예시 |
|------|------|------|
| `qid` | 질문 고유 ID | `edbe6036-eac3-...` |
| `query` | 질문 | "신청자가 사업계획서를 작성할 때 어떤 사항을 반드시 기재해야 합니까?" |
| `retrieval_gt` | 정답 문서 ID 리스트 | `[[33ba42be-...]]` |
| `retrieval_gt_contents` | 정답 문서 내용 | 해당 문서의 텍스트 |
| `generation_gt` | 정답 답변 | "공동대표 또는 각자대표의 경력, 이력 등입니다." |

### Corpus 데이터 (55개 문서)

| 컬럼 | 설명 |
|------|------|
| `doc_id` | 문서 고유 ID |
| `contents` | 문서 내용 (청킹된 텍스트) |
| `path` | 원본 파일 경로 |
| `start_end_idx` | 원본에서의 시작/끝 인덱스 |
| `metadata` | 메타데이터 (날짜, 페이지 등) |

**데이터 출처**: 2024년 글로벌 팁스(Global TIPS) 창업기업 모집 공고문

---

## 실행 전 요구사항

### 1. 환경변수 설정

```bash
export OPENAI_API_KEY="your-api-key"
```

### 2. 가상환경 활성화

```bash
source .venv/bin/activate
```

---

## 실행 결과

실행이 완료되면 `logs/0/` 폴더가 생성됩니다:

```text
logs/0/
├── summary.csv              # 최적 파이프라인 요약
├── retrieve_node_line/
│   └── semantic_retrieval/
│       ├── 0.parquet        # 검색 결과
│       └── best_0.parquet   # 최적 검색 결과
└── post_retrieve_node_line/
    ├── prompt_maker/
    │   └── ...
    └── generator/
        └── ...
```

### 결과 확인

```bash
# 대시보드로 결과 시각화
autorag dashboard --trial_dir logs/0

# 또는 summary.csv 직접 확인
cat logs/0/summary.csv
```

---

## 주의사항

1. **API Rate Limit**: OpenAI API 요청 제한에 걸릴 수 있음 (429 에러)
   - 해결: 잠시 대기 후 재실행, 또는 API 티어 업그레이드

2. **메모리 사용량**: 벡터DB 인덱싱 시 메모리 사용량 증가
   - 권장: 최소 8GB RAM

3. **실행 시간**: 샘플 데이터 기준 약 5-10분 소요 (API 응답 속도에 따라 다름)

---

## 커스텀 LLM/임베딩 서버 사용

OpenAI API 대신 자체 LLM/임베딩 서버를 사용하는 방법입니다. **OpenAI API 키가 필요 없습니다.**

### 설정 파일 (simple_korean_custom.yaml)

```yaml
# 커스텀 LLM/임베딩 서버 사용 설정 (OpenAI API 키 불필요)
# LLM 서버: https://llmserving.surromind.ai
# 임베딩 서버: http://10.10.20.94:8081 (BGE-M3 Infinity)

vectordb:
  - name: custom_vectordb
    db_type: chroma
    client_type: persistent
    path: ${PROJECT_DIR}/logs/chroma
    collection_name: custom_collection
    embedding_batch: 100
    embedding_model:
      - type: openai_like
        model_name: BAAI/bge-m3
        api_base: http://10.10.20.94:8081

node_lines:
  - node_line_name: retrieve_node_line
    nodes:
      - node_type: semantic_retrieval
        strategy:
          metrics: [retrieval_f1, retrieval_recall, retrieval_precision]
        top_k: 3
        modules:
          - module_type: vectordb
            vectordb: custom_vectordb

  - node_line_name: post_retrieve_node_line
    nodes:
      - node_type: prompt_maker
        strategy:
          metrics: [bleu, meteor, rouge]
        modules:
          - module_type: chat_fstring
            prompt:
              - - role: system
                  content: "주어진 passage만을 이용하여 질문에 답하시오."
                - role: user
                  content: "passage: {retrieved_contents}\n\nQuestion: {query}\n\nAnswer:"

      - node_type: generator
        strategy:
          metrics:
            - metric_name: rouge
        modules:
          - module_type: llama_index_llm
            llm: openailike
            model: openai/gpt-oss-120b
            api_base: https://llmserving.surromind.ai/v1
            api_key: ""
            is_chat_model: true
            temperature: 0.1
            timeout: 60
            max_tokens: 4096
```

### 주요 설정 항목

| 항목 | 설명 |
|------|------|
| `embedding_model.type: openai_like` | 커스텀 임베딩 서버 사용 |
| `embedding_model.api_base` | 임베딩 서버 URL (Infinity: `/embeddings`) |
| `llm: openailike` | OpenAI 호환 LLM API 사용 |
| `api_base` | LLM 서버 URL (`/v1` 포함 필요) |
| `is_chat_model: true` | Chat Completions API 사용 (필수) |
| `module_type: chat_fstring` | Chat 형식 프롬프트 생성 (필수) |

### 실행 명령어

```bash
# OpenAI API 키 불필요!
autorag evaluate \
  --config sample_config/rag/korean/non_gpu/simple_korean_custom.yaml \
  --qa_data_path tests/resources/dataset_sample_gen_by_autorag/qa.parquet \
  --corpus_data_path tests/resources/dataset_sample_gen_by_autorag/corpus.parquet \
  --project_dir logs
```

### 커스텀 서버 요구사항

| 서버 | 엔드포인트 | 비고 |
|------|-----------|------|
| **LLM (vLLM)** | `POST /v1/chat/completions` | api_base에 `/v1` 포함 |
| **임베딩 (Infinity)** | `POST /embeddings` | `/v1` prefix 없음 |

### 주의사항

1. **`is_chat_model: true`** 필수 - vLLM은 `/v1/chat/completions`만 지원
2. **`chat_fstring`** 모듈 사용 - Chat 형식 프롬프트 생성 필요
3. **api_base URL**:
   - LLM: `/v1` 포함 (예: `https://server/v1`)
   - 임베딩: `/v1` 제외 (Infinity 기준)

---

## 다음 단계

1. **결과 분석**: `autorag dashboard --trial_dir logs/0`
2. **API 서버 배포**: `autorag run_api --trial_dir logs/0`
3. **커스텀 데이터로 테스트**: 직접 생성한 QA/Corpus 데이터 사용
