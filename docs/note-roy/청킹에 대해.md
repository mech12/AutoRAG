# Chunking (청킹) 가이드

> 출처: 꽃다이(edai) 노션 페이지 "Supporting Chunk Modules"

* [Supporting Chunk Modules](https://edai.notion.site/Supporting-Chunk-Modules-8db803dba2ec4cd0a8789659106e86a3)
* [Supporting Parsing Modules](https://edai.notion.site/)
---

## 1. 청킹이란?

AI(특히 RAG 시스템)가 긴 문서를 효율적으로 처리하기 위해 의미 있는 작은 조각(Chunk)으로 나누는 과정입니다.

**왜 필요한가?**
- **정확도**: 조각이 너무 크면 불필요한 정보가 섞이고, 너무 작으면 문맥을 잃음
- **효율성**: AI가 필요한 정보만 읽어 답변 속도 향상 및 비용 절감

---

## 2. 청킹 방식 비교

### 2.1 기본 방식

| 방식 | 설명 | 장단점 |
|------|------|--------|
| **Character/Token Splitter** | 글자 수나 토큰 수로 분할 | 단순하지만 문맥 끊김 위험 |
| **Recursive Character Splitter** | 문단 → 문장 → 단어 순으로 우선순위 따라 분할 | 가장 널리 사용, 문맥 보존 우수 |
| **Markdown/HTML/Code Splitter** | 문서 형식(제목, 표, 코드 구조) 인식하여 분할 | 형식 유지에 적합 |

### 2.2 Sentence vs Semantic

| 비교 항목 | **Sentence (문장 기반)** | **Semantic (의미 기반)** |
|-----------|--------------------------|--------------------------|
| **기준** | 마침표, 물음표 등 문법 규칙 | AI가 분석한 주제/맥락 변화 지점 |
| **도구** | KoNLPy (한국어), 형태소 분석기 | 임베딩(Embedding) 모델 |
| **장점** | 문장이 깔끔하게 분리됨 | 동일 주제가 한 조각에 모임 (RAG 성능 향상) |
| **단점** | 주제 일관성 미보장 | 계산량 많아 처리 속도 느림 |
| **적합 상황** | 한국어 문서, 문장 보존 중요 시 | 복잡한 주제의 문서, 정교한 검색 필요 시 |

**한 줄 요약**
- **Sentence**: "문법적으로 마침표가 찍힌 곳에서 자른다" (형태 기반)
- **Semantic**: "주제가 바뀌는 지점에서 자른다" (의미 기반)

### 2.3 한국어 특화: KoNLPy

한국어는 영어와 구조가 달라 일반적인 AI 도구(TikToken 등)로 자르면 문장이 이상하게 끊길 수 있습니다. KoNLPy 형태소 분석기를 사용하면 한국어 문장을 정확하게 분리할 수 있습니다.

### 2.4 SemanticDoubleMerging

2단계 처리 방식:
1. **Initial Split**: 일단 잘게 나눔
2. **Merging**: 의미가 비슷한 조각들을 다시 합침

단순히 자르기만 할 때 발생하는 "정보의 파편화"를 방지합니다.

---

## 3. LlamaIndex vs LangChain

| 구분 | **LlamaIndex** | **LangChain** |
|------|----------------|---------------|
| **핵심 키워드** | 검색(Retrieval), 데이터 연결, 인덱싱 | 체인(Chain), 도구 연동, 에이전트 |
| **역할** | AI가 자료를 똑똑하게 찾아보게 함 | AI가 여러 단계를 거쳐 작업 수행 |
| **비유** | 도서관 사서, 백과사전 | 맥가이버 칼, 만능 도구상자 |
| **최적 상황** | "내 PDF에서 정확한 답변 찾아줘" | "답변 찾고 이메일로 보내고 요약해줘" |

### 선택 기준

**LlamaIndex 선택:**
- 보유 데이터(PDF, 사내 위키 등)가 많고 정확한 답변이 핵심일 때
- 복잡한 인덱싱이나 Semantic 검색 최적화 필요 시

**LangChain 선택:**
- AI가 여러 도구(웹 검색, 계산기, API)를 사용해야 할 때
- 대화 맥락(Memory) 관리가 중요할 때
- 복잡한 논리 흐름의 에이전트 구축 시

---

## 4. LlamaIndex + LangChain 연동

두 라이브러리는 **보완 관계**로, 함께 사용하면 효과적입니다.

### 연동 방식

1. **LlamaIndex**: 지식 데이터베이스(Vector Store) 구축
2. **도구화**: `LlamaIndexTool`로 LangChain이 인식할 수 있게 변환
3. **LangChain 에이전트**: 자료 조사 필요 시 LlamaIndex 도구 호출

### 장점

- **최고의 검색 성능**: LlamaIndex의 정교한 인덱싱/청킹 기술 활용
- **복잡한 작업 수행**: LangChain으로 외부 API 호출, 조건부 로직 처리
- **유지보수 용이**: 데이터 관리 로직과 서비스 운영 로직 분리

### 주의사항

- **단순한 Q&A 봇**: 하나만 사용하는 것이 효율적 (오버엔지니어링 방지)
- **성능 최적화**: 두 라이브러리 연동 시 데이터 전달 단계 증가로 미세한 속도 저하 가능

---

## 요약

> **"데이터 조사는 LlamaIndex가, 전체 흐름 관리는 LangChain이 담당"**

데이터의 성격(일반 글, 코드, 표 등)에 맞춰 최적의 청킹 방식을 선택하고, 필요에 따라 두 프레임워크를 조합하여 사용합니다.
