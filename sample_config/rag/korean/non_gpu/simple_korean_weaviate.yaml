# =============================================================================
# AutoRAG 커스텀 서버 설정 파일 (Weaviate Vector DB)
# =============================================================================
# 이 설정은 Weaviate Vector DB를 사용합니다.
# Weaviate는 하이브리드 검색 (Vector + BM25)과 GraphQL API를 지원합니다.
#
# 환경변수 설정 필요 (.env 파일):
#   - CUSTOM_LLM_API_BASE: LLM 서버 URL
#   - CUSTOM_LLM_MODEL: LLM 모델 이름
#   - CUSTOM_LLM_API_KEY: API 키
#   - CUSTOM_EMBEDDING_API_BASE: 임베딩 서버 URL
#   - CUSTOM_EMBEDDING_MODEL: 임베딩 모델
#   - WEAVIATE_HOST: Weaviate 서버 호스트
#   - WEAVIATE_HTTP_PORT: Weaviate HTTP 포트
#   - WEAVIATE_GRPC_PORT: Weaviate gRPC 포트
#
# 실행 방법:
#   export $(cat .env | xargs)
#   autorag evaluate \
#     --config sample_config/rag/korean/non_gpu/simple_korean_weaviate.yaml \
#     --qa_data_path tests/resources/dataset_sample_gen_by_autorag/qa.parquet \
#     --corpus_data_path tests/resources/dataset_sample_gen_by_autorag/corpus.parquet \
#     --project_dir logs
# =============================================================================

# =============================================================================
# 1단계: 벡터 데이터베이스 설정 (Weaviate)
# =============================================================================
vectordb:
  - name: weaviate_vectordb
    db_type: weaviate
    client_type: docker
    host: ${WEAVIATE_HOST}
    port: ${WEAVIATE_HTTP_PORT}
    grpc_port: ${WEAVIATE_GRPC_PORT}
    collection_name: autorag_collection
    embedding_batch: 100
    similarity_metric: cosine

    # 임베딩 모델 설정
    embedding_model:
      - type: openai_like
        model_name: ${CUSTOM_EMBEDDING_MODEL}
        api_base: ${CUSTOM_EMBEDDING_API_BASE}

# =============================================================================
# 2단계: RAG 파이프라인 정의 (node_lines)
# =============================================================================
node_lines:

  # ===========================================================================
  # 2-1. 검색 노드 라인 (Retrieve Node Line)
  # ===========================================================================
  - node_line_name: retrieve_node_line
    nodes:

      # -----------------------------------------------------------------------
      # 의미 기반 검색 (Semantic Retrieval)
      # -----------------------------------------------------------------------
      - node_type: semantic_retrieval

        strategy:
          metrics:
            - retrieval_f1
            - retrieval_recall
            - retrieval_precision

        top_k: 3

        modules:
          - module_type: vectordb
            vectordb: weaviate_vectordb

  # ===========================================================================
  # 2-2. 후처리 노드 라인 (Post-Retrieve Node Line)
  # ===========================================================================
  - node_line_name: post_retrieve_node_line
    nodes:

      # -----------------------------------------------------------------------
      # 프롬프트 생성기 (Prompt Maker)
      # -----------------------------------------------------------------------
      - node_type: prompt_maker

        strategy:
          metrics:
            - bleu
            - meteor
            - rouge

        modules:
          - module_type: chat_fstring

            prompt:
              - - role: system
                  content: "주어진 passage만을 이용하여 질문에 답하시오."
                - role: user
                  content: "passage: {retrieved_contents}\n\nQuestion: {query}\n\nAnswer:"

      # -----------------------------------------------------------------------
      # 답변 생성기 (Generator)
      # -----------------------------------------------------------------------
      - node_type: generator

        strategy:
          metrics:
            - metric_name: rouge

        modules:
          - module_type: llama_index_llm

            llm: openailike
            model: ${CUSTOM_LLM_MODEL}
            api_base: ${CUSTOM_LLM_API_BASE}
            api_key: ${CUSTOM_LLM_API_KEY}

            is_chat_model: true
            temperature: 0.1
            timeout: 60
            max_tokens: 4096
