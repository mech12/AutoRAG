# =============================================================================
# AutoRAG 커스텀 서버 설정 파일
# =============================================================================
# 이 설정은 OpenAI API 대신 자체 LLM/임베딩 서버를 사용합니다.
# OpenAI API 키가 필요 없습니다.
#
# 환경변수 설정 필요 (.env 파일):
#   - CUSTOM_LLM_API_BASE: LLM 서버 URL (예: https://llmserving.example.com/v1)
#   - CUSTOM_LLM_MODEL: LLM 모델 이름 (예: openai/gpt-oss-120b)
#   - CUSTOM_LLM_API_KEY: API 키 (자체 서버는 빈 문자열)
#   - CUSTOM_EMBEDDING_API_BASE: 임베딩 서버 URL (예: http://localhost:8081)
#   - CUSTOM_EMBEDDING_MODEL: 임베딩 모델 (예: BAAI/bge-m3)
#
# 환경변수 설정 방법:
#   1. .env.example을 .env로 복사: cp .env.example .env
#   2. .env 파일에서 서버 URL 수정
#   3. 환경변수 로드 후 실행
#
# 실행 방법:
#   # 환경변수 로드 (bash)
#   export $(cat .env | xargs)
#
#   # 평가 실행
#   make quick-test-custom
#   또는
#   autorag evaluate \
#     --config sample_config/rag/korean/non_gpu/simple_korean_custom.yaml \
#     --qa_data_path tests/resources/dataset_sample_gen_by_autorag/qa.parquet \
#     --corpus_data_path tests/resources/dataset_sample_gen_by_autorag/corpus.parquet \
#     --project_dir logs
# =============================================================================

# =============================================================================
# 1단계: 벡터 데이터베이스 설정
# =============================================================================
# 문서를 벡터로 변환하여 저장하는 데이터베이스를 정의합니다.
# 의미 기반 검색(Semantic Retrieval)에 사용됩니다.
vectordb:
  - name: custom_vectordb           # 이 이름으로 아래 modules에서 참조
    db_type: chroma                 # 벡터DB 종류 (chroma, milvus, pinecone 등)
    client_type: persistent         # persistent: 디스크 저장, ephemeral: 메모리만
    path: ${PROJECT_DIR}/logs/chroma  # ChromaDB 저장 경로
    collection_name: custom_collection  # 컬렉션 이름
    embedding_batch: 100            # 한 번에 임베딩할 문서 수

    # 임베딩 모델 설정
    # 문서와 쿼리를 벡터로 변환하는 모델
    embedding_model:
      - type: openai_like           # OpenAI API 호환 서버 사용
        model_name: ${CUSTOM_EMBEDDING_MODEL}  # 환경변수에서 모델명 로드
        api_base: ${CUSTOM_EMBEDDING_API_BASE}  # 환경변수에서 URL 로드 (/v1 없음!)

# =============================================================================
# 2단계: RAG 파이프라인 정의 (node_lines)
# =============================================================================
# node_lines는 순차적으로 실행되는 노드 그룹입니다.
# 각 node_line 내의 nodes도 순차적으로 실행됩니다.
#
# 파이프라인 흐름:
#   Query → [retrieve_node_line] → [post_retrieve_node_line] → Answer
#            └─ semantic_retrieval   ├─ prompt_maker
#                                    └─ generator
node_lines:

  # ===========================================================================
  # 2-1. 검색 노드 라인 (Retrieve Node Line)
  # ===========================================================================
  # 사용자 질문과 관련된 문서를 검색합니다.
  - node_line_name: retrieve_node_line
    nodes:

      # -----------------------------------------------------------------------
      # 의미 기반 검색 (Semantic Retrieval)
      # -----------------------------------------------------------------------
      # 쿼리를 벡터로 변환하고, 유사한 문서 벡터를 찾아 반환합니다.
      - node_type: semantic_retrieval

        # 평가 전략: 검색 성능을 측정할 메트릭들
        strategy:
          metrics:
            - retrieval_f1         # F1 점수 (정밀도와 재현율의 조화평균)
            - retrieval_recall     # 재현율 (정답 문서 중 검색된 비율)
            - retrieval_precision  # 정밀도 (검색 결과 중 정답 비율)

        top_k: 3  # 상위 3개 문서 검색

        # 검색 모듈 설정
        modules:
          - module_type: vectordb
            vectordb: custom_vectordb  # 위에서 정의한 벡터DB 사용

  # ===========================================================================
  # 2-2. 후처리 노드 라인 (Post-Retrieve Node Line)
  # ===========================================================================
  # 검색된 문서를 바탕으로 프롬프트를 생성하고 답변을 생성합니다.
  - node_line_name: post_retrieve_node_line
    nodes:

      # -----------------------------------------------------------------------
      # 프롬프트 생성기 (Prompt Maker)
      # -----------------------------------------------------------------------
      # 검색된 문서와 질문을 조합하여 LLM에 전달할 프롬프트를 만듭니다.
      - node_type: prompt_maker

        # 평가 전략: 프롬프트 품질 측정 (생성된 답변과 정답 비교)
        strategy:
          metrics:
            - bleu    # BLEU 점수 (n-gram 일치도)
            - meteor  # METEOR 점수 (의미적 유사도 포함)
            - rouge   # ROUGE 점수 (요약 품질)

        modules:
          # chat_fstring: Chat 형식 프롬프트 생성
          # 중요! vLLM은 /v1/chat/completions만 지원하므로 chat_fstring 필수
          - module_type: chat_fstring

            # 프롬프트 템플릿 (OpenAI Chat 형식)
            # {query}: 사용자 질문으로 대체됨
            # {retrieved_contents}: 검색된 문서 내용으로 대체됨
            prompt:
              - - role: system
                  content: "주어진 passage만을 이용하여 질문에 답하시오."
                - role: user
                  content: "passage: {retrieved_contents}\n\nQuestion: {query}\n\nAnswer:"

      # -----------------------------------------------------------------------
      # 답변 생성기 (Generator)
      # -----------------------------------------------------------------------
      # 프롬프트를 LLM에 전달하여 최종 답변을 생성합니다.
      - node_type: generator

        # 평가 전략: 생성된 답변의 품질 측정
        strategy:
          metrics:
            - metric_name: rouge  # ROUGE 점수로 정답과 비교

        modules:
          # llama_index_llm: LlamaIndex의 LLM 래퍼 사용
          - module_type: llama_index_llm

            # LLM 설정 (환경변수에서 로드)
            llm: openailike                    # OpenAI 호환 API 사용
            model: ${CUSTOM_LLM_MODEL}         # 환경변수에서 모델명 로드
            api_base: ${CUSTOM_LLM_API_BASE}   # 환경변수에서 URL 로드 (/v1 필수!)
            api_key: ${CUSTOM_LLM_API_KEY}     # 환경변수에서 API 키 로드

            # 중요 설정
            is_chat_model: true          # Chat API 사용 (/v1/chat/completions)
                                         # vLLM은 이 옵션이 필수!

            # 생성 파라미터
            temperature: 0.1             # 낮을수록 일관된 답변 (0.0~1.0)
            timeout: 60                  # API 타임아웃 (초)
            max_tokens: 4096             # 최대 생성 토큰 수
